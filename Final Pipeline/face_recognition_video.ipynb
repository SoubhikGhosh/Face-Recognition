{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition of faces from Inception Resnet V1 ( Vedio Stream Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection through MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Face detector\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Recognition through Facenet pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face recognition model (FaceNet pre-trained)\n",
    "facenet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "def extract_face(image, mtcnn, target_size=(160, 160)):\n",
    "    \"\"\"\n",
    "    Detects and crops the face from the image.\n",
    "    \"\"\"\n",
    "    boxes, _ = mtcnn.detect(image)\n",
    "    if boxes is None:\n",
    "        return None\n",
    "    cropped_faces = []\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = [int(b) for b in box]\n",
    "        face = image[y1:y2, x1:x2]\n",
    "        face_resized = cv2.resize(face, target_size)\n",
    "        cropped_faces.append(face_resized)\n",
    "    return cropped_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_faces(faces, facenet):\n",
    "    \"\"\"\n",
    "    Generates embeddings for detected faces.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for face in faces:\n",
    "        # Convert to PyTorch tensor\n",
    "        face = torch.tensor(face.transpose(2, 0, 1)).float().unsqueeze(0) / 255.0\n",
    "        embedding = facenet(face).detach().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_face_database(image_folder, mtcnn, facenet):\n",
    "    \"\"\"\n",
    "    Creates a database of known faces from images in the given folder.\n",
    "    \"\"\"\n",
    "    database = {}\n",
    "    for file in os.listdir(image_folder):\n",
    "        name, ext = os.path.splitext(file)\n",
    "        if ext.lower() not in ['.jpg', '.png']:\n",
    "            continue\n",
    "        image = cv2.imread(os.path.join(image_folder, file))\n",
    "        faces = extract_face(image, mtcnn)\n",
    "        if faces:\n",
    "            embeddings = encode_faces(faces, facenet)\n",
    "            database[name] = embeddings[0]\n",
    "    return database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces_faiss(input_image, database, mtcnn, facenet, threshold):\n",
    "    \"\"\"\n",
    "    Recognizes faces in the input image by comparing with the database using FAISS.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_image: The image to process.\n",
    "    - database: A dictionary of precomputed embeddings (name -> embedding).\n",
    "    - mtcnn: The face detector.\n",
    "    - facenet: The face recognition model.\n",
    "    - threshold: Similarity threshold for recognition.\n",
    "    \n",
    "    Returns:\n",
    "    - Annotated image with recognition results.\n",
    "    \"\"\"\n",
    "    # Extract and crop faces\n",
    "    faces = extract_face(input_image, mtcnn)\n",
    "    if not faces:\n",
    "        return input_image, []\n",
    "\n",
    "    # Generate embeddings for detected faces\n",
    "    embeddings = encode_faces(faces, facenet)\n",
    "    \n",
    "    # Prepare FAISS index\n",
    "    # Convert database to a format suitable for FAISS\n",
    "    db_embeddings = np.array(list(database.values())).astype('float32')\n",
    "    db_names = list(database.keys())\n",
    "    \n",
    "    # Create FAISS index for L2 similarity (use IndexFlatIP for cosine similarity)\n",
    "    index = faiss.IndexFlatIP(db_embeddings.shape[1])  # Inner Product for cosine similarity\n",
    "    faiss.normalize_L2(db_embeddings)  # Normalize embeddings for cosine similarity\n",
    "    index.add(db_embeddings)\n",
    "\n",
    "    \n",
    "    # Normalize face embeddings for cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Search database for nearest matches\n",
    "    distances, indices = index.search(embeddings, 1)  # Top-1 match for each embedding\n",
    "\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[:, 0], indices[:, 0]):\n",
    "        if dist > threshold and idx < len(db_names):\n",
    "            results.append((db_names[idx], dist))\n",
    "        else:\n",
    "            results.append((None, dist))\n",
    "    \n",
    "    # Annotate image\n",
    "    annotated_image = input_image.copy()\n",
    "    for (box, (name, score)) in zip(mtcnn.detect(input_image)[0], results):\n",
    "        x1, y1, x2, y2 = [int(b) for b in box]\n",
    "        label = f\"{name} ({score:.2f})\" if name else \"Unknown\"\n",
    "        color = (0, 255, 0) if name else (0, 0, 255)\n",
    "        cv2.rectangle(annotated_image, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(annotated_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.25, color, 1)\n",
    "    \n",
    "    return annotated_image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces_video(video_path, database, mtcnn, facenet, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Recognizes faces in a video by comparing with the database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open video capture\n",
    "        cap = cv2.VideoCapture(video_path if video_path else 0)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(\"Failed to open video source.\")\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or frame is None:\n",
    "                print(\"End of video or failed to capture frame.\")\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                # Call the face recognition function\n",
    "                annotated_image_faiss, results_faiss = recognize_faces_faiss(frame, database, mtcnn, facenet, threshold)\n",
    "                if annotated_image_faiss is None:\n",
    "                    continue\n",
    "\n",
    "                # Display the annotated frame\n",
    "                cv2.imshow(\"Face Recognition\", annotated_image_faiss)\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            # Press 'q' to exit the video loop\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        # Release resources and close windows\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        for _ in range(1, 5):  # Ensure all windows are destroyed\n",
    "            cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method keys of dict object at 0x2a4053540>\n"
     ]
    }
   ],
   "source": [
    "# Load known faces\n",
    "database = build_face_database(\"known_faces\", mtcnn, facenet)\n",
    "print(database.keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 14:42:36.220 Python[22818:19962132] WARNING: Secure coding is automatically enabled for restorable state! However, not on all supported macOS versions of this application. Opt-in to secure coding explicitly by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState:.\n"
     ]
    }
   ],
   "source": [
    "# Recognize faces from video or webcam\n",
    "video_path = None  # Replace with video path or leave as None for webcam\n",
    "recognize_faces_video(video_path, database, mtcnn, facenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import nmslib\n",
    "\n",
    "def recognize_faces_nmslib(input_image, database, mtcnn, facenet, threshold):\n",
    "    \"\"\"\n",
    "    Recognizes faces in the input image by comparing with the database using NMSLIB.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_image: The image to process.\n",
    "    - database: A dictionary of precomputed embeddings (name -> embedding).\n",
    "    - mtcnn: The face detector.\n",
    "    - facenet: The face recognition model.\n",
    "    - threshold: Similarity threshold for recognition.\n",
    "    \n",
    "    Returns:\n",
    "    - Annotated image with recognition results.\n",
    "    \"\"\"\n",
    "    # Extract and crop faces\n",
    "    faces = extract_face(input_image, mtcnn)\n",
    "    if not faces:\n",
    "        return input_image, []\n",
    "\n",
    "    # Generate embeddings for detected faces\n",
    "    embeddings = encode_faces(faces, facenet)\n",
    "    \n",
    "    # Prepare NMSLIB index\n",
    "    # Convert database to a format suitable for NMSLIB\n",
    "    db_embeddings = np.array(list(database.values())).astype('float32')\n",
    "    db_names = list(database.keys())\n",
    "\n",
    "    # Create NMSLIB index\n",
    "    index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "    for idx, embedding in enumerate(db_embeddings):\n",
    "        index.addDataPoint(idx, embedding)\n",
    "    index.createIndex({'post': 2}, print_progress=False)\n",
    "\n",
    "    results = []\n",
    "    for embedding in embeddings:\n",
    "        # Query the nearest neighbor\n",
    "        nearest_neighbors = index.knnQuery(embedding, k=1)\n",
    "        idx, dist = nearest_neighbors[0][0], 1 - nearest_neighbors[1][0]  # Convert to similarity score\n",
    "        if dist > threshold and idx < len(db_names):\n",
    "            results.append((db_names[idx], dist))\n",
    "        else:\n",
    "            results.append((None, dist))\n",
    "    \n",
    "    # Annotate image\n",
    "    annotated_image = input_image.copy()\n",
    "    for (box, (name, score)) in zip(mtcnn.detect(input_image)[0], results):\n",
    "        x1, y1, x2, y2 = [int(b) for b in box]\n",
    "        label = f\"{name} ({score:.2f})\" if name else \"Unknown\"\n",
    "        color = (0, 255, 0) if name else (0, 0, 255)\n",
    "        cv2.rectangle(annotated_image, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(annotated_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.25, color, 1)\n",
    "    \n",
    "    return annotated_image, results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
