{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rPPG : Remote photoplethysmography\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advancements in medical technologies have led to the development of contact-free methods of haemodynamic monitoring such as remote photoplethysmography (rPPG). rPPG uses video cameras to interpret variations in skin colour related to blood flow, which are analysed to generate vital signs readings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liveness Detection with Multiple Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (2.18.0)\n",
      "Requirement already satisfied: keras in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (3.7.0)\n",
      "Requirement already satisfied: numpy in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Requirement already satisfied: mediapipe in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (0.10.18)\n",
      "Requirement already satisfied: scipy in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (1.13.1)\n",
      "Requirement already satisfied: opencv-python in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: dlib in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (19.24.6)\n",
      "Requirement already satisfied: matplotlib in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (3.9.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from keras) (0.13.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from mediapipe) (24.2.0)\n",
      "Requirement already satisfied: jax in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.55.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from jax->mediapipe) (8.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: pycparser in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/soubhikghosh/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow keras numpy mediapipe scipy opencv-python dlib matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt, find_peaks\n",
    "import dlib  # For landmark detection\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optical Flow for Motion Detection\n",
    "\n",
    "This function computes the optical flow between consecutive frames, a crucial step in detecting motion in video streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optical flow between consecutive frames\n",
    "def compute_optical_flow(prev_gray, frame_gray):\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, frame_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eye Blink Detection with dlib's Landmarks\n",
    "\n",
    "This section identifies eye blinks using facial landmarks and computes the eye aspect ratio (EAR) to detect if the eyes are closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute eye aspect ratio for blink detection\n",
    "def eye_aspect_ratio(eye):\n",
    "    eye = np.array([(point.x, point.y) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Detect eye blinks based on landmarks\n",
    "def detect_blinks(landmarks, eye_thresh=0.3):\n",
    "    left_eye = [landmarks.part(i) for i in range(36, 42)]\n",
    "    right_eye = [landmarks.part(i) for i in range(42, 48)]\n",
    "    left_eye_ratio = eye_aspect_ratio(left_eye)\n",
    "    right_eye_ratio = eye_aspect_ratio(right_eye)\n",
    "    return left_eye_ratio < eye_thresh or right_eye_ratio < eye_thresh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head Pose Estimation\n",
    "\n",
    "Uses dlib facial landmarks to estimate head pose based on a 3D model of the human face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_head_pose(landmarks):\n",
    "    # 2D image points (for head pose estimation)\n",
    "    image_points = np.array([\n",
    "        (landmarks[30].x, landmarks[30].y),  # Nose tip\n",
    "        (landmarks[8].x, landmarks[8].y),    # Chin\n",
    "        (landmarks[36].x, landmarks[36].y),  # Left eye left corner\n",
    "        (landmarks[45].x, landmarks[45].y),  # Right eye right corner\n",
    "        (landmarks[48].x, landmarks[48].y),  # Left mouth corner\n",
    "        (landmarks[54].x, landmarks[54].y)   # Right mouth corner\n",
    "    ], dtype=\"double\")\n",
    "\n",
    "    # 3D model points for head pose estimation\n",
    "    model_points = np.array([\n",
    "        (0.0, 0.0, 0.0),       # Nose tip\n",
    "        (0.0, -330.0, -65.0),  # Chin\n",
    "        (-225.0, 170.0, -135.0),  # Left eye left corner\n",
    "        (225.0, 170.0, -135.0),   # Right eye right corner\n",
    "        (-150.0, -150.0, -125.0), # Left mouth corner\n",
    "        (150.0, -150.0, -125.0)   # Right mouth corner\n",
    "    ])\n",
    "\n",
    "    # Camera internals\n",
    "    size = (640, 480)\n",
    "    focal_length = size[0]\n",
    "    center = (size[0] / 2, size[1] / 2)\n",
    "    camera_matrix = np.array([\n",
    "        [focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    dist_coeffs = np.zeros((4, 1))  # Assuming no lens distortion\n",
    "\n",
    "    _, rotation_vector, translation_vector = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs)\n",
    "\n",
    "    return rotation_vector, translation_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandpass Filtering for rPPG Signal\n",
    "\n",
    "Applies a bandpass filter to the remote photoplethysmography (rPPG) signal for heart rate estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def bandpass_filter(data, lowcut=0.7, highcut=4.0, fs=30, order=5):\n",
    "    if len(data) < 2 * order:\n",
    "        return data\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order)\n",
    "    return filtfilt(b, a, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart Rate Estimation from rPPG Signal\n",
    "\n",
    "Extracts the rPPG signal, applies a filter, and estimates the heart rate using peak detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_roi(frame, face_cascade):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "    return faces\n",
    "\n",
    "def extract_rppg_signal(roi):\n",
    "    roi = cv2.resize(roi, (100, 100))  # Resize to standard dimensions\n",
    "    green_channel = roi[:, :, 1]  # Extract the green channel\n",
    "    rppg_signal = np.mean(green_channel, axis=0)  # Compute mean across rows\n",
    "    return rppg_signal\n",
    "\n",
    "def estimate_heart_rate(rppg_signal, fps, window_size=60, lowcut=0.7, highcut=4.0):\n",
    "    if len(rppg_signal) < window_size:\n",
    "        return None, None\n",
    "\n",
    "    windowed_signal = rppg_signal[-window_size:]\n",
    "    filtered_signal = bandpass_filter(windowed_signal, lowcut=lowcut, highcut=highcut, fs=fps)\n",
    "\n",
    "    peaks, _ = find_peaks(filtered_signal, distance=15)\n",
    "    if len(peaks) > 1:\n",
    "        heart_rate = len(peaks) / (len(filtered_signal) / fps) * 60\n",
    "        return heart_rate, filtered_signal\n",
    "    return None, None\n",
    "\n",
    "def compute_variability(filtered_signal):\n",
    "    if filtered_signal is None or len(filtered_signal) < 2:\n",
    "        return 0\n",
    "    # Variability is the standard deviation of the heart rate signal\n",
    "    return np.std(filtered_signal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motion Thresholding\n",
    "\n",
    "This detects motion by comparing consecutive frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motion_threshold(roi, previous_roi, motion_threshold=0.01):\n",
    "    \"\"\"Calculate motion based on previous ROI and current frame\"\"\"\n",
    "    if previous_roi is None:\n",
    "        return False\n",
    "\n",
    "    # Resize both ROIs to the same size\n",
    "    roi_resized = cv2.resize(roi, (previous_roi.shape[1], previous_roi.shape[0]))\n",
    "\n",
    "    # Convert both ROIs to grayscale\n",
    "    roi_gray = cv2.cvtColor(roi_resized, cv2.COLOR_BGR2GRAY)\n",
    "    previous_roi_gray = cv2.cvtColor(previous_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Compute absolute difference between the current and previous frame\n",
    "    diff = cv2.absdiff(previous_roi_gray, roi_gray)\n",
    "\n",
    "    # Calculate motion score by summing pixel differences and normalizing by ROI size\n",
    "    motion_score = np.sum(diff) / float(roi_resized.size)\n",
    "\n",
    "    return motion_score > motion_threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Skin texture and subject focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_texture(roi):\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    texture_score = laplacian.var()  # Variance of Laplacian\n",
    "    return texture_score > 30, texture_score  # Lower texture variance threshold for real faces\n",
    "\n",
    "def analyze_focus(roi):\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    focus_score = laplacian.var()  # Variance of Laplacian\n",
    "    return focus_score > 10, focus_score  # Looser focus threshold to allow more real faces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liveness Evaluation based on all the variables\n",
    "\n",
    "By assigning weights to each aspect discussed before we can generate a liveness score and process frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_liveness(heart_rate, variability, motion_detected, texture_score, focus_detected, blink_detected, confidence_threshold=0.6):\n",
    "    \"\"\"Evaluate liveness based on heart rate, motion, texture, focus, and blink detection\"\"\"\n",
    "    motion_weight = 0.15\n",
    "    texture_weight = 0.15\n",
    "    heart_rate_weight = 0.25\n",
    "    focus_weight = 0.2\n",
    "    blink_weight = 0.25\n",
    "\n",
    "    alive_count = 0\n",
    "    if motion_detected:\n",
    "        alive_count += 1\n",
    "    if texture_score:\n",
    "        alive_count += 1\n",
    "    if heart_rate and 50 <= heart_rate <= 100:\n",
    "        alive_count += 1\n",
    "    if focus_detected:\n",
    "        alive_count += 1\n",
    "    if blink_detected:\n",
    "        alive_count += 1\n",
    "\n",
    "    score = (motion_weight * motion_detected) + (texture_weight * texture_score) + (heart_rate_weight * (heart_rate is not None and 50 <= heart_rate <= 100)) + (focus_weight * focus_detected) + (blink_weight * blink_detected)\n",
    "\n",
    "    if (alive_count >= 3 or score >= 40) and variability > 20 :  # Deepfake variability threshold\n",
    "        return \"Alive\", True\n",
    "    return \"Deep Fake\", False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-Time Liveness Detection\n",
    "Captures video from the webcam and applies all the above techniques to detect liveness in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sum_v = 0\n",
    "sum_f = 0\n",
    "sum_t = 0\n",
    "frame_count = 0\n",
    "\n",
    "try:\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    fps = 30\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Error: Could not open webcam.\")\n",
    "\n",
    "    previous_roi = None\n",
    "    previous_gray = None\n",
    "    previous_landmarks = None\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi = frame[y:y+h, x:x+w]\n",
    "            if previous_roi is not None:\n",
    "                motion_detected = motion_threshold(roi, previous_roi)\n",
    "            else:\n",
    "                motion_detected = False\n",
    "\n",
    "            # Use dlib to detect landmarks and blink detection\n",
    "            landmarks = predictor(gray, dlib.rectangle(x, y, x+w, y+h))\n",
    "            blink_detected = detect_blinks(landmarks)\n",
    "            \n",
    "            # Estimate heart rate from RPPG signal\n",
    "            rppg_signal = extract_rppg_signal(roi)\n",
    "            heart_rate, filtered_signal = estimate_heart_rate(rppg_signal, fps)\n",
    "            variability = compute_variability(rppg_signal)\n",
    "            sum_v += variability\n",
    "\n",
    "            frame_count+=1\n",
    "\n",
    "            # Analyze texture and focus\n",
    "            texture_score, texture = analyze_texture(roi)\n",
    "            sum_t+=texture\n",
    "\n",
    "            focus_detected, focus = analyze_focus(roi)\n",
    "            sum_f+=focus\n",
    "\n",
    "            # Evaluate liveness\n",
    "            status, is_alive = evaluate_liveness(heart_rate, variability, motion_detected, texture_score, focus_detected, blink_detected)\n",
    "            print(f\"Status: {status}, Heart Rate: {heart_rate}, Variability: {variability}\")\n",
    "\n",
    "            previous_roi = roi\n",
    "            previous_gray = gray\n",
    "            previous_landmarks = landmarks\n",
    "\n",
    "            # Draw rectangle around the face\n",
    "            color = (0, 255, 0) if is_alive else (0, 0, 255)  # Green for alive, red for deep fake\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "            # Display heart rate and liveness status on the frame\n",
    "            # cv2.putText(frame, f\"Heart Rate: {heart_rate if heart_rate else 'N/A'} bpm\", (x, y - 30),\n",
    "            #             cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "            cv2.putText(frame, f\"Liveness: {status}\", (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "            # Display the resulting frame\n",
    "            cv2.imshow('Frame', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print (f\"average variability: {sum_v/frame_count}\")\n",
    "            print (f\"average focus: {sum_f/frame_count}\")\n",
    "            print (f\"average texture: {sum_t/frame_count}\")\n",
    "\n",
    "            break\n",
    "\n",
    "except:\n",
    "    print(\"Exception\")\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
