{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ailia\n",
      "  Downloading ailia-1.5.0.0-py3-none-macosx_12_0_universal2.whl.metadata (1.2 kB)\n",
      "Downloading ailia-1.5.0.0-py3-none-macosx_12_0_universal2.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: ailia\n",
      "Successfully installed ailia-1.5.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ailia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'arg_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# import original modules\u001b[39;00m\n\u001b[1;32m     13\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../util\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01marg_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_base_parser, update_parser, get_savepath  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_and_download_models  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize_image  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'arg_utils'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from logging import getLogger\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import ailia\n",
    "\n",
    "# import original modules\n",
    "sys.path.append('../../util')\n",
    "from arg_utils import get_base_parser, update_parser, get_savepath  # noqa\n",
    "from model_utils import check_and_download_models  # noqa\n",
    "from image_utils import normalize_image  # noqa\n",
    "from detector_utils import load_image  # noqa\n",
    "from webcamera_utils import get_capture, get_writer  # noqa\n",
    "\n",
    "import draw_utils\n",
    "from detection_utils import face_detection\n",
    "from detection_utils import IMAGE_SIZE as IMAGE_DET_SIZE\n",
    "from blendshape import face_blendshapes, plot_face_blendshapes_bar_graph\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "# ======================\n",
    "# Parameters\n",
    "# ======================\n",
    "\n",
    "WEIGHT_PATH = 'face_landmarks_detector.onnx'\n",
    "MODEL_PATH = 'face_landmarks_detector.onnx.prototxt'\n",
    "WEIGHT_DET_PATH = 'face_detector.onnx'\n",
    "MODEL_DET_PATH = 'face_detector.onnx.prototxt'\n",
    "WEIGHT_BLENDSHAPE_PATH = 'face_blendshapes.onnx'\n",
    "MODEL_BLENDSHAPE_PATH = 'face_blendshapes.onnx.prototxt'\n",
    "REMOTE_PATH = 'https://storage.googleapis.com/ailia-models/facemesh_v2/'\n",
    "\n",
    "IMAGE_PATH = 'demo.jpg'\n",
    "SAVE_IMAGE_PATH = 'output.png'\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "NUM_LANDMARKS = 478\n",
    "\n",
    "ROI = namedtuple('ROI', ['x_center', 'y_center', 'width', 'height', 'rotation'])\n",
    "\n",
    "# ======================\n",
    "# Arguemnt Parser Config\n",
    "# ======================\n",
    "\n",
    "parser = get_base_parser(\n",
    "    'FaceMesh-V2', IMAGE_PATH, SAVE_IMAGE_PATH\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--blendshape', action=\"store_true\",\n",
    "    help=\"visualize the face blendshapes categories using a bar graph.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--onnx',\n",
    "    action='store_true',\n",
    "    help='execute onnxruntime version.'\n",
    ")\n",
    "args = update_parser(parser)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Secondaty Functions\n",
    "# ======================\n",
    "\n",
    "def draw_result(img, face_landmarks):\n",
    "    # Draw the face landmarks.\n",
    "\n",
    "    draw_utils.draw_landmarks(\n",
    "        image=img,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=draw_utils.FACEMESH_TESSELATION,\n",
    "        connection_drawing_spec=draw_utils.get_tesselation_style())\n",
    "\n",
    "    draw_utils.draw_landmarks(\n",
    "        image=img,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=draw_utils.FACEMESH_CONTOURS,\n",
    "        connection_drawing_spec=draw_utils.get_contours_style())\n",
    "\n",
    "    draw_utils.draw_landmarks(\n",
    "        image=img,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=draw_utils.FACEMESH_IRISES,\n",
    "        connection_drawing_spec=draw_utils.get_iris_connections_style())\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Main functions\n",
    "# ======================\n",
    "\n",
    "def warp_perspective(\n",
    "        img, roi: ROI,\n",
    "        dst_width, dst_height,\n",
    "        keep_aspect_ratio=True):\n",
    "    im_h, im_w, _ = img.shape\n",
    "\n",
    "    v_pad = h_pad = 0\n",
    "    if keep_aspect_ratio:\n",
    "        dst_aspect_ratio = dst_height / dst_width\n",
    "        roi_aspect_ratio = roi.height / roi.width\n",
    "\n",
    "        if dst_aspect_ratio > roi_aspect_ratio:\n",
    "            new_height = roi.width * dst_aspect_ratio\n",
    "            new_width = roi.width\n",
    "            v_pad = (1 - roi_aspect_ratio / dst_aspect_ratio) / 2\n",
    "        else:\n",
    "            new_width = roi.height / dst_aspect_ratio\n",
    "            new_height = roi.height\n",
    "            h_pad = (1 - dst_aspect_ratio / roi_aspect_ratio) / 2\n",
    "\n",
    "        roi = ROI(roi.x_center, roi.y_center, new_width, new_height, roi.rotation)\n",
    "\n",
    "    a = roi.width\n",
    "    b = roi.height\n",
    "    c = math.cos(roi.rotation)\n",
    "    d = math.sin(roi.rotation)\n",
    "    e = roi.x_center\n",
    "    f = roi.y_center\n",
    "    g = 1 / im_w\n",
    "    h = 1 / im_h\n",
    "\n",
    "    project_mat = [\n",
    "        [a * c * g, -b * d * g, 0.0, (-0.5 * a * c + 0.5 * b * d + e) * g],\n",
    "        [a * d * h, b * c * h, 0.0, (-0.5 * b * c - 0.5 * a * d + f) * h],\n",
    "        [0.0, 0.0, a * g, 0.0],\n",
    "        [0.0, 0.0, 0.0, 1.0],\n",
    "    ]\n",
    "\n",
    "    rotated_rect = (\n",
    "        (roi.x_center, roi.y_center),\n",
    "        (roi.width, roi.height),\n",
    "        roi.rotation * 180. / math.pi\n",
    "    )\n",
    "    pts1 = cv2.boxPoints(rotated_rect)\n",
    "\n",
    "    pts2 = np.float32([[0, dst_height], [0, 0], [dst_width, 0], [dst_width, dst_height]])\n",
    "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "    img = cv2.warpPerspective(\n",
    "        img, M, (dst_width, dst_height), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n",
    "\n",
    "    return img, project_mat, roi, (h_pad, v_pad)\n",
    "\n",
    "\n",
    "def preprocess_det(img):\n",
    "    im_h, im_w, _ = img.shape\n",
    "\n",
    "    \"\"\"\n",
    "    resize & padding\n",
    "    \"\"\"\n",
    "    roi = ROI(0.5 * im_w, 0.5 * im_h, im_w, im_h, 0)\n",
    "    dst_width = dst_height = IMAGE_DET_SIZE\n",
    "    img, matrix, *_ = warp_perspective(\n",
    "        img, roi,\n",
    "        dst_width, dst_height)\n",
    "\n",
    "    \"\"\"\n",
    "    normalize & reshape\n",
    "    \"\"\"\n",
    "    img = normalize_image(img, normalize_type='127.5')\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img.astype(np.float32)\n",
    "\n",
    "    return img, matrix\n",
    "\n",
    "\n",
    "def preprocess(img, roi):\n",
    "    im_h, im_w, _ = img.shape\n",
    "\n",
    "    \"\"\"\n",
    "    resize & padding\n",
    "    \"\"\"\n",
    "    dst_width = dst_height = IMAGE_SIZE\n",
    "    img, _, roi, pad = warp_perspective(\n",
    "        img, roi,\n",
    "        dst_width, dst_height,\n",
    "        keep_aspect_ratio=False)\n",
    "\n",
    "    img = normalize_image(img, normalize_type='255')\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img.astype(np.float32)\n",
    "\n",
    "    return img, roi, pad\n",
    "\n",
    "\n",
    "def post_processing(input_tensors, roi, pad):\n",
    "    num_landmarks = NUM_LANDMARKS\n",
    "    num_dimensions = 3\n",
    "\n",
    "    # TensorsToFaceLandmarksGraph\n",
    "    input_tensors = input_tensors.reshape(-1)\n",
    "    output_landmarks = np.zeros((num_landmarks, num_dimensions))\n",
    "    for i in range(num_landmarks):\n",
    "        offset = i * num_dimensions\n",
    "        output_landmarks[i] = input_tensors[offset:offset + 3]\n",
    "\n",
    "    norm_landmarks = output_landmarks / 256\n",
    "\n",
    "    # LandmarkLetterboxRemovalCalculator\n",
    "    h_pad, v_pad = pad\n",
    "    left = h_pad\n",
    "    top = v_pad\n",
    "    left_and_right = h_pad * 2\n",
    "    top_and_bottom = v_pad * 2\n",
    "    for landmark in norm_landmarks:\n",
    "        new_x = (landmark[0] - left) / (1 - left_and_right)\n",
    "        new_y = (landmark[1] - top) / (1 - top_and_bottom)\n",
    "        new_z = landmark[2] / (1 - left_and_right)  # Scale Z coordinate as X.\n",
    "        landmark[:3] = (new_x, new_y, new_z)\n",
    "\n",
    "    # LandmarkProjectionCalculator\n",
    "    width = roi.width\n",
    "    height = roi.height\n",
    "    x_center = roi.x_center\n",
    "    y_center = roi.y_center\n",
    "    angle = roi.rotation\n",
    "    for landmark in norm_landmarks:\n",
    "        x = landmark[0] - 0.5\n",
    "        y = landmark[1] - 0.5\n",
    "        z = landmark[2]\n",
    "        new_x = math.cos(angle) * x - math.sin(angle) * y\n",
    "        new_y = math.sin(angle) * x + math.cos(angle) * y\n",
    "\n",
    "        new_x = new_x * width + x_center\n",
    "        new_y = new_y * height + y_center\n",
    "        new_z = z * width\n",
    "\n",
    "        landmark[...] = new_x, new_y, new_z\n",
    "\n",
    "    return norm_landmarks\n",
    "\n",
    "\n",
    "def predict(models, img):\n",
    "    im_h, im_w, _ = img.shape\n",
    "    img = img[:, :, ::-1]  # BGR -> RGB\n",
    "\n",
    "    input, matrix = preprocess_det(img)\n",
    "\n",
    "    # feedforward\n",
    "    det_net = models['det_net']\n",
    "    if not args.onnx:\n",
    "        output = det_net.predict([input])\n",
    "    else:\n",
    "        output = det_net.run(None, {'input': input})\n",
    "    detections, scores = output\n",
    "\n",
    "    boxes, scores = face_detection(detections, scores, matrix)\n",
    "    if len(boxes) == 0:\n",
    "        return np.zeros((0, NUM_LANDMARKS, 3))\n",
    "\n",
    "    landmarks_list = []\n",
    "    for box in boxes:\n",
    "        # DetectionsToRectsCalculator\n",
    "        rect_width = box[2] - box[0]\n",
    "        rect_height = box[3] - box[1]\n",
    "        center_x = (box[0] + box[2]) / 2\n",
    "        center_y = (box[1] + box[3]) / 2\n",
    "\n",
    "        x0, y0 = box[4] * im_w, box[5] * im_h\n",
    "        x1, y1 = box[6] * im_w, box[7] * im_h\n",
    "        angle = 0 - math.atan2(-(y1 - y0), x1 - x0)\n",
    "        angle = angle - 2 * math.pi * math.floor((angle - (-math.pi)) / (2 * math.pi));\n",
    "\n",
    "        # RectTransformationCalculator\n",
    "        scale_x = scale_y = 1.5\n",
    "        rect_width = rect_width * scale_x\n",
    "        rect_height = rect_height * scale_y\n",
    "\n",
    "        roi = ROI(\n",
    "            center_x * im_w, center_y * im_h,\n",
    "            rect_width * im_w, rect_height * im_h,\n",
    "            angle)\n",
    "        img, roi, pad = preprocess(img, roi)\n",
    "\n",
    "        # feedforward\n",
    "        net = models['net']\n",
    "        if not args.onnx:\n",
    "            output = net.predict([img])\n",
    "        else:\n",
    "            output = net.run(None, {'input_12': img})\n",
    "        landmark_tensors, presence_flag_tensors, _ = output\n",
    "\n",
    "        norm_rect = ROI(\n",
    "            roi.x_center / im_w, roi.y_center / im_h,\n",
    "            roi.width / im_w, roi.height / im_h,\n",
    "            angle)\n",
    "        landmarks = post_processing(landmark_tensors, norm_rect, pad)\n",
    "        landmarks_list.append(landmarks)\n",
    "\n",
    "    landmarks = np.stack(landmarks_list, axis=0)\n",
    "\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "def recognize_from_image(models):\n",
    "    # input image loop\n",
    "    for image_path in args.input:\n",
    "        logger.info(image_path)\n",
    "\n",
    "        # prepare input data\n",
    "        img = load_image(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "        # inference\n",
    "        logger.info('Start inference...')\n",
    "        if args.benchmark:\n",
    "            logger.info('BENCHMARK mode')\n",
    "            total_time_estimation = 0\n",
    "            for i in range(args.benchmark_count):\n",
    "                start = int(round(time.time() * 1000))\n",
    "                detection_result = predict(models, img)\n",
    "                end = int(round(time.time() * 1000))\n",
    "                estimation_time = (end - start)\n",
    "\n",
    "                # Logging\n",
    "                logger.info(f'\\tailia processing estimation time {estimation_time} ms')\n",
    "                if i != 0:\n",
    "                    total_time_estimation = total_time_estimation + estimation_time\n",
    "\n",
    "            logger.info(f'\\taverage time estimation {total_time_estimation / (args.benchmark_count - 1)} ms')\n",
    "        else:\n",
    "            detection_result = predict(models, img)\n",
    "\n",
    "        res_img = img\n",
    "        for detection in detection_result:\n",
    "            res_img = draw_result(res_img, detection)\n",
    "\n",
    "        # plot result\n",
    "        savepath = get_savepath(args.savepath, image_path, ext='.png')\n",
    "        logger.info(f'saved at : {savepath}')\n",
    "        cv2.imwrite(savepath, res_img)\n",
    "\n",
    "        if args.blendshape and len(detection_result) > 0:\n",
    "            bls_net = models['blendshape']\n",
    "            score = face_blendshapes(bls_net, detection_result[0], img.shape[:2], args.onnx)\n",
    "            img = plot_face_blendshapes_bar_graph(score)\n",
    "\n",
    "            cv2.imwrite(\"bar_graph.png\", img)\n",
    "\n",
    "    logger.info('Script finished successfully.')\n",
    "\n",
    "\n",
    "def recognize_from_video(models):\n",
    "    capture = get_capture(args.video)\n",
    "\n",
    "    # create video writer if savepath is specified as video format\n",
    "    f_h = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    f_w = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "    if args.savepath != SAVE_IMAGE_PATH:\n",
    "        writer = get_writer(args.savepath, f_h, f_w + f_h)\n",
    "    else:\n",
    "        writer = None\n",
    "\n",
    "    frame_shown = False\n",
    "    while True:\n",
    "        ret, frame = capture.read()\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')) or not ret:\n",
    "            break\n",
    "        if frame_shown and cv2.getWindowProperty('frame', cv2.WND_PROP_VISIBLE) == 0:\n",
    "            break\n",
    "\n",
    "        detection_result = predict(models, frame)\n",
    "\n",
    "        visual_img = frame\n",
    "        for detection in detection_result:\n",
    "            visual_img = draw_result(visual_img, detection)\n",
    "\n",
    "        if args.blendshape and len(detection_result) > 0:\n",
    "            bls_net = models['blendshape']\n",
    "            score = face_blendshapes(bls_net, detection_result[0], frame.shape[:2], args.onnx)\n",
    "            bar_img = plot_face_blendshapes_bar_graph(score)\n",
    "            bar_img = cv2.resize(bar_img, (f_h, f_h))\n",
    "\n",
    "            packed_img = np.zeros((f_h, f_w + f_h, 3), dtype=np.uint8)\n",
    "            packed_img[:,0:f_w,:] = visual_img\n",
    "            packed_img[:,f_w:f_w+f_h,:] = bar_img[:,:,0:3]\n",
    "\n",
    "            visual_img = packed_img\n",
    "\n",
    "        cv2.imshow('frame', visual_img)\n",
    "\n",
    "        frame_shown = True\n",
    "\n",
    "        # save results\n",
    "        if writer is not None:\n",
    "            writer.write(visual_img)\n",
    "\n",
    "    capture.release()\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "\n",
    "    logger.info('Script finished successfully.')\n",
    "\n",
    "\n",
    "def main():\n",
    "    check_and_download_models(WEIGHT_PATH, MODEL_PATH, REMOTE_PATH)\n",
    "    check_and_download_models(WEIGHT_DET_PATH, MODEL_DET_PATH, REMOTE_PATH)\n",
    "    if args.blendshape:\n",
    "        check_and_download_models(WEIGHT_BLENDSHAPE_PATH, MODEL_BLENDSHAPE_PATH, REMOTE_PATH)\n",
    "\n",
    "    env_id = args.env_id\n",
    "\n",
    "    # initialize\n",
    "    bls_net = None\n",
    "    if not args.onnx:\n",
    "        net = ailia.Net(MODEL_PATH, WEIGHT_PATH, env_id=env_id)\n",
    "        det_net = ailia.Net(MODEL_DET_PATH, WEIGHT_DET_PATH, env_id=env_id)\n",
    "        if args.blendshape:\n",
    "            bls_net = ailia.Net(MODEL_BLENDSHAPE_PATH, WEIGHT_BLENDSHAPE_PATH, env_id=env_id)\n",
    "    else:\n",
    "        import onnxruntime\n",
    "        cuda = 0 < ailia.get_gpu_environment_id()\n",
    "        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if cuda else ['CPUExecutionProvider']\n",
    "        net = onnxruntime.InferenceSession(WEIGHT_PATH, providers=providers)\n",
    "        det_net = onnxruntime.InferenceSession(WEIGHT_DET_PATH, providers=providers)\n",
    "        if args.blendshape:\n",
    "            bls_net = onnxruntime.InferenceSession(WEIGHT_BLENDSHAPE_PATH, providers=providers)\n",
    "\n",
    "    models = {\n",
    "        \"net\": net,\n",
    "        \"det_net\": det_net,\n",
    "        \"blendshape\": bls_net,\n",
    "    }\n",
    "\n",
    "    if args.video is not None:\n",
    "        recognize_from_video(models)\n",
    "    else:\n",
    "        recognize_from_image(models)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
